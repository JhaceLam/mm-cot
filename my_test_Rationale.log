args Namespace(bs=8, caption_file='data/instruct_captions.json', data_root='data', epoch=20, eval_acc=10, eval_bs=4, eval_le=None, evaluate_dir='models/MM-CoT-UnifiedQA-base-Rationale', final_eval=True, img_type='detr', input_len=512, lr=5e-05, model='allenai/unifiedqa-t5-base', options=['A', 'B', 'C', 'D', 'E'], output_dir='experiments', output_len=512, pass_evaluate=False, pass_predict_eval=False, pass_predict_test=False, prompt_format='QCM-LE', seed=42, test_le=None, test_split='test', train_split='train', use_caption=False, use_generate=False, use_small_set=True, user_msg='rationale', val_split='val')
====Input Arguments====
{
  "data_root": "data",
  "output_dir": "experiments",
  "model": "allenai/unifiedqa-t5-base",
  "options": [
    "A",
    "B",
    "C",
    "D",
    "E"
  ],
  "epoch": 20,
  "lr": 5e-05,
  "bs": 8,
  "input_len": 512,
  "output_len": 512,
  "eval_bs": 4,
  "eval_acc": 10,
  "train_split": "train",
  "val_split": "val",
  "test_split": "test",
  "use_generate": false,
  "final_eval": true,
  "user_msg": "rationale",
  "img_type": "detr",
  "eval_le": null,
  "test_le": null,
  "evaluate_dir": "models/MM-CoT-UnifiedQA-base-Rationale",
  "caption_file": "data/instruct_captions.json",
  "use_caption": false,
  "prompt_format": "QCM-LE",
  "seed": 42,
  "use_small_set": true,
  "pass_evaluate": false,
  "pass_predict_test": false,
  "pass_predict_eval": false
}
img_features size:  (11208, 100, 256)
number of train problems: 12726

number of val problems: 4241

number of test problems: 4241

[17:17:23] [Model]: Loading models/MM-CoT-UnifiedQA-base-Rationale...                                                                                                                                                                main.py:74
                                                                                                                                                                                                                                               
           [Data]: Reading data...                                                                                                                                                                                                   main.py:75
                                                                                                                                                                                                                                               
********************
Warning: Small test_set and val_set are being used!
********************
model parameters:  226643712
Traceback (most recent call last):
  File "main.py", line 443, in <module>
    T5Trainer(
  File "main.py", line 298, in T5Trainer
    batch_metrics = trainer.evaluate(eval_dataset=batch, max_length=args.output_len)
  File "/root/miniconda3/lib/python3.8/site-packages/transformers/trainer_seq2seq.py", line 159, in evaluate
    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
  File "/root/miniconda3/lib/python3.8/site-packages/transformers/trainer.py", line 3043, in evaluate
    output = eval_loop(
  File "/root/miniconda3/lib/python3.8/site-packages/transformers/trainer.py", line 3235, in evaluation_loop
    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
  File "/root/miniconda3/lib/python3.8/site-packages/transformers/trainer_seq2seq.py", line 276, in prediction_step
    generated_tokens = self.model.generate(**inputs, **gen_kwargs)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.8/site-packages/transformers/generation/utils.py", line 1611, in generate
    return self.beam_search(
  File "/root/miniconda3/lib/python3.8/site-packages/transformers/generation/utils.py", line 2909, in beam_search
    outputs = self(
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/mm-cot-v1/model.py", line 115, in forward
    image_embedding = self.image_dense(image_ids)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
TypeError: linear(): argument 'input' (position 1) must be Tensor, not NoneType
